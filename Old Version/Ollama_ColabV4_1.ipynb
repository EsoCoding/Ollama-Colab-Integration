{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luxadevi/Ollama-Colab-Integration/blob/main/Ollama_ColabV4_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore the Advanced Ollama-Companion:\n",
        "# Streamlit Enhanced\n",
        "---\n",
        "\n",
        "Welcome to the latest version of Ollama-Companion, now enhanced with Streamlit for a more interactive and intuitive user experience. As the developer behind this innovative project, I'm excited to introduce a suite of new features that redefine how you interact with and manage language models.\n",
        "\n",
        "Key Innovations in Ollama-Companion:\n",
        "1. Quantization of Huggingface Models via UI\n",
        "The foremost feature of Ollama-Companion is the ability to quantize Huggingface models through a user-friendly interface. This functionality allows you to efficiently convert models into different formats, catering to a variety of computational needs.\n",
        "\n",
        "2. Dynamic Module Integration\n",
        "Seamlessly integrate a variety of modules as defined in shared.py, ensuring a modular and scalable approach to application development.\n",
        "\n",
        "3. Streamlit-Powered User Interface\n",
        "The UI has been revamped using Streamlit to enhance intuitiveness and responsiveness, making it easier to navigate and interact with various features.\n",
        "\n",
        "4. Enhanced Model Interaction Features\n",
        "Modelfile Manager: Beyond model selection, this feature lets you delve into the details of each model, providing options to view, manage, and even delete model files as required.\n",
        "Interactive Modelfile Creator: Customize your model files in real-time, offering enhanced control and flexibility.\n",
        "5. Chat Interface with LLAVA Image Analysis\n",
        "The chat interface is equipped with LLAVA for image recognition and analysis, adding a dynamic and interactive dimension to your conversations with language models.\n",
        "\n",
        "6. Advanced Configuration Tools\n",
        "Ollama API Configurator: Manage Ollama API endpoints directly from the UI.\n",
        "LiteLLM Proxy and Public Endpoint: Easily set up proxies and public endpoints, ensuring secure and efficient model sharing.\n",
        "7. Efficient Model Management Systems\n",
        "Fast Model Downloading: Download models from Huggingface with improved speed.\n",
        "Quantization Options: Choose between high or medium precision GGUF formats for model transformation.\n",
        "Secure Model Uploads to Huggingface: Upload models to Huggingface confidently with enhanced security measures.\n",
        "8. Security Enhancements\n",
        "Token Encryption: Protect your Huggingface token with an additional layer of encryption for increased data security."
      ],
      "metadata": {
        "id": "SIujJTVa7hAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to run\n",
        "* Run the cells below by clicking the play button to download  and start ollama with a public url .  \n",
        "Want to run Ollama companion locally?  \n",
        "\n",
        "**Use this command to isntall local**:\n",
        "\n",
        "\n",
        "```\n",
        "curl https://raw.githubusercontent.com/Luxadevi/Ollama-Companion/main/install.sh | sh\n",
        "```\n",
        "\n",
        "#### Faster installer:\n",
        "\n",
        "\n",
        "Dont want to compile Llama.cpp for quanting use -minimal as install flag, skips compiling and  guarantees a faster install.\n",
        "\n",
        "Need a more info/verbose install?  \n",
        "Remove `2>` and `1>&2` from commands"
      ],
      "metadata": {
        "id": "3gQuQak3Ckh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/Luxadevi/Ollama-Companion.git 2> /dev/null 1>&2\n",
        "print(\"Cloning Ollama-Companion from git...\")\n",
        "# Install virtualenv\n",
        "!sudo apt install virtualenv 2> /dev/null 1>&2\n",
        "print(\"Installing some dependencies, please hold on...\")\n",
        "# Convert Windows line endings to Unix line endings in the install.sh script\n",
        "!sed -i 's/\\r//' /content/Ollama-Companion/install.sh 2> /dev/null 1>&2\n",
        "print(\"Converting line endings in install script...\")\n",
        "# Make the script executable and run it\n",
        "!chmod +x /content/Ollama-Companion/install.sh 2> /dev/null 1>&2\n",
        "print(\"Running the installation script and compiling Llama.cpp...\")\n",
        "!/content/Ollama-Companion/install.sh 2> /dev/null\n",
        "# Installer also starts Ollama-Companion directly\n",
        "\n",
        "#To empty cache and some potential storage\n",
        "# Purge pip cache and clean up temporary files\n",
        "!pip cache purge 2> /dev/null 1>&2\n",
        "!find /tmp -type f -atime +1 -delete 2> /dev/null 1>&2\n",
        "!sudo apt-get clean 2> /dev/null 1>&2\n",
        "!sudo apt-get autoremove 2> /dev/null 1>&2"
      ],
      "metadata": {
        "id": "gQSkK1w82y52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If stopped or wanting to relaunch.**   \n",
        "Run this cell to restart your LLM stack"
      ],
      "metadata": {
        "id": "9QADK7qltjVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/Ollama-Companion/start.sh"
      ],
      "metadata": {
        "id": "aCwfO2OT6W_8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
